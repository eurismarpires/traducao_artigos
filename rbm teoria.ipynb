{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricted Boltzmann Machines, and neural networks in general, work by updating the states of some neurons given the states of others, so let's talk about how the states of individual units change. Assuming we know the connection weights in our RBM (we'll explain how to learn these below), to update the state of unit $i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>As máquinas de Boltzmann restrito e as redes neurais em geral, funcionam atualizando os estados de alguns neurônios dados os estados dos outros, então vamos falar sobre como os estados das unidades individuais mudam. Supondo que conheçamos os pesos de conexão em nosso RBM (vamos explicar como aprender isso abaixo), para atualizar o estado da unidade $ i $:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the activation energy $a_i = \\sum_j w_{ij} x_j$ of unit $i$, where the sum runs over all units $j$ that unit $i$ is connected to, $w_{ij}$ is the weight of the connection between $i$ and $j$, and $x_j$ is the 0 or 1 state of unit $j$. In other words, all of unit $i$'s neighbors send it a message, and we compute the sum of all these messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"blue\">Calcule a energia de ativação $a_i = \\sum_j w_{ij} x_j$ da unidade $i$, onde a soma é executada em todas as unidades $j$ essa unidade $i$ está conectada a, $w_{ij}$ é o peso da conexão entre $i$ e $j$ e $x_j$ é o 0 ou 1 estado da unidade $j$. Em outras palavras, todos os vizinhos da unidade $i$ enviam uma mensagem e calculamos a soma de todas essas mensagens.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let $p_i = \\sigma(a_i)$, where $\\sigma(x) = \\frac{1}{1 + exp(-x)}$ is the logistic function. Note that $p_i$ is close to 1 for large positive activation energies, and $p_i$ is close to 0 for negative activation energies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>\n",
    "Deixe $p_i = \\sigma(a_i)$, onde $\\sigma(x) = \\frac{1}{1 + exp(-x)}$ é a função logística. Observe que $p_i$ é próximo de 1 para grandes energias de ativação positivas e $p_i$ é próximo de 0 para energias de ativação negativas.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We then turn unit $i$ on with probability $p_i$, and turn it off with probability $1 - p_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Em seguida, ativamos a unidade $ i $ com probabilidade $ p_i $, e desligue com probabilidade $ 1 - p_i $.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (In layman's terms, units that are positively connected to each other try to get each other to share the same state (i.e., be both on or off), while units that are negatively connected to each other are enemies that prefer to be in different states.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>(Em termos de leigos, as unidades que estão positivamente conectadas entre si tentam obter um para o outro para compartilhar o mesmo estado (ou seja, estarem ativadas ou desativadas), enquanto as unidades que estão negativamente conectadas entre si são inimigos que preferem estar em diferentes Estados.) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's suppose our two hidden units really do correspond to SF/fantasy and Oscar winners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Por exemplo, suponhamos que nossas duas unidades escondidas realmente correspondem com SF / fantasia e vencedores do Oscar.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If Alice has told us her six binary preferences on our set of movies, we could then ask our RBM which of the hidden units her preferences activate (i.e., ask the RBM to explain her preferences in terms of latent factors). So the six movies send messages to the hidden units, telling them to update themselves. (Note that even if Alice has declared she wants to watch Harry Potter, Avatar, and LOTR 3, this doesn't guarantee that the SF/fantasy hidden unit will turn on, but only that it will turn on with high probability. This makes a bit of sense: in the real world, Alice wanting to watch all three of those movies makes us highly suspect she likes SF/fantasy in general, but there's a small chance she wants to watch them for other reasons. Thus, the RBM allows us to generate models of people in the messy, real world.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* <font color=blue>Se Alice nos contou suas seis preferências binárias em nosso conjunto de filmes, poderíamos então perguntar ao nosso RBM quais das unidades escondidas que suas preferências ativam (ou seja, pedir ao RBM para explicar suas preferências em termos de fatores latentes). Assim, os seis filmes enviam mensagens para as unidades escondidas, dizendo-lhes para se atualizar. (Note que, mesmo que Alice tenha declarado que quer assistir a Harry Potter, Avatar e LOTR 3, isso não garante que a unidade escondida SF / fantasia se aumente, mas apenas que ele irá ativar com alta probabilidade . Isso faz com que Um pouco de sentido: no mundo real, Alice quer assistir a todos esses filmes nos faz altamente suspeitar que ela gosta de SF / fantasia em geral, mas há uma pequena chance de que ela os assista por outros motivos. Assim,\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "Conversely, if we know that one person likes SF/fantasy (so that the SF/fantasy unit is on), we can then ask the RBM which of the movie units that hidden unit turns on (i.e., ask the RBM to generate a set of movie recommendations). So the hidden units send messages to the movie units, telling them to update their states. (Again, note that the SF/fantasy unit being on doesn't guarantee that we'll always recommend all three of Harry Potter, Avatar, and LOTR 3 because, hey, not everyone who likes science fiction liked Avatar.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "<font color=blue>\n",
    "Por outro lado, se sabemos que uma pessoa gosta de SF / fantasia (para que a unidade de SF / fantasia esteja ligada), podemos pedir ao RBM quais das unidades de filme em que a unidade escondida gira (ou seja, peça ao RBM que gere um conjunto Das recomendações do filme). Então, as unidades escondidas enviam mensagens às unidades de filme, informando-lhes para atualizar seus estados. (Mais uma vez, note que a unidade de SF / fantasia que está sendo ativada não garante que sempre recomendamos os três Harry Potter, Avatar e LOTR 3 porque, hey, nem todo mundo que gosta de ficção científica gostava de Avatar).\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we learn the connection weights in our network? Suppose we have a bunch of training examples, where each training example is a binary vector with six elements corresponding to a user's movie preferences. Then for each epoch, do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "Então, como aprendemos os pesos de conexão em nossa rede? Suponha que temos um monte de exemplos de treinamento, onde cada exemplo de treinamento é um vetor binário com seis elementos correspondentes às preferências de filme de um usuário. Então, para cada época, faça o seguinte:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Take a training example (a set of six movie preferences). Set the states of the visible units to these preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Faça um exemplo de treino (um conjunto de seis preferências de filmes). Defina os estados das unidades visíveis para essas preferências.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, update the states of the hidden units using the logistic activation rule described above: for the $j$th hidden unit, compute its activation energy $a_j = \\sum_i w_{ij} x_i$, and set $x_j$ to 1 with probability $\\sigma(a_j)$ and to 0 with probability $1 - \\sigma(a_j)$. Then for each edge $e_{ij}$, compute $Positive(e_{ij}) = x_i * x_j$ (i.e., for each pair of units, measure whether they're both on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Em seguida, atualize os estados das unidades ocultas usando a regra de ativação logística descrita acima: para a unidade escondida $j$ th, computa sua energia de ativação $a_j = \\sum_i w_{ij} x_i$ e defina $ x_j $ para 1 com Probabilidade $\\sigma(a_j)$ e para 0 com probabilidade $1 - \\sigma(a_j)$. Em seguida, para cada aresta $e_{ij} $, calcule $Positivo(e_{ij}) = x_i * x_j$ (ou seja, para cada par de unidades, mire se eles estão ambos).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now reconstruct the visible units in a similar manner: for each visible unit, compute its activation energy $a_i$, and update its state. (Note that this reconstruction may not match the original preferences.) Then update the hidden units again, and compute $Negative(e_{ij}) = x_i * x_j$ for each edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Agora, reconstrua as unidades visíveis de forma semelhante: para cada unidade visível, computa sua energia de ativação $ a_i $ e atualiza seu estado. (Observe que essa reconstrução pode não corresponder às preferências originais). Em seguida, atualize as unidades escondidas novamente e compute $ Negativo (e_ {ij}) = x_i * x_j $ para cada uma das bordas.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update the weight of each edge $e_{ij}$ by setting $w_{ij} = w_{ij} + L * (Positive(e_{ij}) - Negative(e_{ij}))$, where $L$ is a learning rate.\n",
    "Repeat over all training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Atualize o peso de cada borda $ e_ {ij} $ ajustando $ w_ {ij} = w_ {ij} + L * (Positivo (e_ {ij}) - Negativo (e_ {ij})) $, onde $ L $ É uma taxa de aprendizado.\n",
    "Repita todos os exemplos de treino.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue until the network converges (i.e., the error between the training examples and their reconstructions falls below some threshold) or we reach some maximum number of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue até que a rede converge (ou seja, o erro entre os exemplos de treino e suas reconstruções cai abaixo de um limite) ou chegamos a um número máximo de épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this update rule make sense? Note that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Por que essa regra de atualização faz sentido? Observe que</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the first phase, $Positive(e_{ij})$ measures the association between the $i$th and $j$th unit that we want the network to learn from our training examples;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Na primeira fase, $ Positive(e_{ij}) $ mede a associação entre a unidade $i$ esima e e $j$ esima que queremos que a rede aprenda com nossos exemplos de treinamento;</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the \"reconstruction\" phase, where the RBM generates the states of visible units based on its hypotheses about the hidden units alone, $Negative(e_{ij})$ measures the association that the network itself generates (or \"daydreams\" about) when no units are fixed to training dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Na fase de \"reconstrução\", onde o RBM gera estados de unidades visíveis com base em suas hipóteses sobre as unidades escondidas sozinhas, $Negative (e_{ij})$ mede a associação que a própria rede gera (ou \"daydreams\") Quando nenhuma unidade é fixada em dados de treinamento.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by adding $Positive(e_{ij}) - Negative(e_{ij})$ to each edge weight, we're helping the network's daydreams better match the reality of our training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Então, adicionando $ Positivo(e_{ij}) - Negativo(e_{ij}) $ para cada peso de ponta, estamos ajudando os devaneios da rede a combinar melhor a realidade de nossos exemplos de treinamento.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You may hear this update rule called contrastive divergence, which is basically a funky term for \"approximate gradient descent\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>(Você pode ouvir esta regra de atualização chamada divergência contrastiva , que é basicamente um termo funky para \"descida de gradiente aproximada\").</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a simple RBM implementation in Python (the code is heavily commented, so take a look if you're still a little fuzzy on how everything works), so let's use it to walk through some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Eu escrevi uma implementação RBM simples no Python (o código é bastante comentado, então veja se você ainda está um pouco distorcido sobre como tudo funciona), então vamos usá-lo para caminhar por alguns exemplos.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I trained the RBM using some fake data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Primeiro, treinei o RBM usando alguns dados falsos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alice: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.\n",
    "* Bob: (Harry Potter = 1, Avatar = 0, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). SF/fantasy fan, but doesn't like Avatar.\n",
    "* Carol: (Harry Potter = 1, Avatar = 1, LOTR 3 = 1, Gladiator = 0, Titanic = 0, Glitter = 0). Big SF/fantasy fan.\n",
    "* David: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan.\n",
    "* Eric: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Oscar winners fan, except for Titanic.\n",
    "* Fred: (Harry Potter = 0, Avatar = 0, LOTR 3 = 1, Gladiator = 1, Titanic = 1, Glitter = 0). Big Oscar winners fan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network learned the following weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>A rede aprendeu os seguintes pesos:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|              |   Bias Unit   |    Hidden 1   |     Hidden 2   |\n",
    "|---------------------------------------------------------------                 \n",
    "|Bias Unit     |  -0.08257658  |   -0.19041546 |     1.57007782 |\n",
    "|Harry Potter  |  -0.82602559  |   -7.08986885 |     4.96606654 |\n",
    "|Avatar        |  -1.84023877  |   -5.18354129 |     2.27197472 |\n",
    "|LOTR 3        |   3.92321075  |    2.51720193 |     4.11061383 |\n",
    "|Gladiator     |   0.10316995  |    6.74833901 |    -4.00505343 |\n",
    "|Titanic       |  -0.97646029  |    3.25474524 |    -5.59606865 |\n",
    "|Glitter       |  -4.44685751  |   -2.81563804 |    -2.91540988 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first hidden unit seems to correspond to the Oscar winners, and the second hidden unit seems to correspond to the SF/fantasy movies, just as we were hoping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Note-se que a primeira unidade escondida parece corresponder aos vencedores do Oscar e a segunda unidade escondida parece corresponder aos filmes SF / fantasy, assim como esperávamos.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we give the RBM a new user, George, who has (Harry Potter = 0, Avatar = 0, LOTR 3 = 0, Gladiator = 1, Titanic = 1, Glitter = 0) as his preferences? It turns the Oscar winners unit on (but not the SF/fantasy unit), correctly guessing that George probably likes movies that are Oscar winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>O que acontece se darmos ao RBM um novo usuário, George, que tem (Harry Potter = 0, Avatar = 0, LOTR 3 = 0, Gladiator = 1, Titanic = 1, Glitter = 0) como suas preferências? Vira a unidade dos vencedores do Oscar (mas não a SF / unidade de fantasia), adivinendo corretamente que George provavelmente gosta de filmes que são vencedores do Oscar.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we activate only the SF/fantasy unit, and run the RBM a bunch of different times? In my trials, it turned on Harry Potter, Avatar, and LOTR 3 three times; it turned on Avatar and LOTR 3, but not Harry Potter, once; and it turned on Harry Potter and LOTR 3, but not Avatar, twice. Note that, based on our training examples, these generated preferences do indeed match what we might expect real SF/fantasy fans want to watch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>O que acontece se ativarmos apenas a unidade de SF / fantasia e executar o RBM um monte de tempos diferentes? Em minhas provações, ele ativou Harry Potter, Avatar e LOTR 3 três vezes; Ativou Avatar e LOTR 3, mas não Harry Potter, uma vez; E ativou Harry Potter e LOTR 3, mas não Avatar, duas vezes. Note-se que, com base em nossos exemplos de treinamento, essas preferências geradas coincidem com o que poderíamos esperar que os fãs reais de SF / fantasia queiram assistir.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to keep the connection-learning algorithm I described above pretty simple, so here are some modifications that often appear in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Eu tentei manter o algoritmo de aprendizagem de conexão que eu descrevi acima, bastante simples, então aqui estão algumas modificações que muitas vezes aparecem na prática:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Above, $Negative(e_{ij})$ was determined by taking the product of the $i$th and $j$th units after reconstructing the visible units once and then updating the hidden units again. We could also take the product after some larger number of reconstructions (i.e., repeat updating the visible units, then the hidden units, then the visible units again, and so on); this is slower, but describes the network's daydreams more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Acima, $ Negative (e_{ij}) $ foi determinado tomando o produto das unidades $ i $ th e $ j $ th depois de reconstruir as unidades visíveis uma vez e depois atualizar as unidades escondidas novamente. Poderíamos também levar o produto depois de um número maior de reconstruções (ou seja, repetir a atualização das unidades visíveis, depois as unidades escondidas, as unidades visíveis novamente, etc.); Isso é mais lento, mas descreve os devaneios da rede com mais precisão.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of using $Positive(e_{ij})=x_i * x_j$, where $x_i$ and $x_j$ are binary 0 or 1 states, we could also let $x_i$ and/or $x_j$ be activation probabilities. Similarly for $Negative(e_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Em vez de usar $ Positivo (e_{ij}) = x_i * x_j $, onde $ x_i $ e $ x_j $ são 0 ou 1 estados binários , também podemos permitir que $ x_i $ e / ou $ x_j $ sejam probabilidades de ativação . Da mesma forma por $ Negativo (e_{ij}) $.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We could penalize larger edge weights, in order to get a sparser or more regularized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Poderíamos penalizar pesos de arestas maiores, para obter um modelo mais disperso ou mais regularizado</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When updating edge weights, we could use a momentum factor: we would add to each edge a weighted sum of the current step as described above (i.e., $L * (Positive(e_{ij}) - Negative(e_{ij})$) and the step previously taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Ao atualizar pesos de aresta, poderíamos usar um fator de momentum: nós adicionaríamos a cada uma das bordas uma soma ponderada da etapa atual conforme descrito acima (ou seja, $ L * (Positivo (e_{ij}) - Negativo (e_{ij}) $) E o passo previamente tomado.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of using only one training example in each epoch, we could use batches of examples in each epoch, and only update the network's weights after passing through all the examples in the batch. This can speed up the learning by taking advantage of fast matrix-multiplication algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue>Em vez de usar apenas um exemplo de treinamento em cada época, podemos usar lotes de exemplos em cada época, e apenas atualizar os pesos da rede depois de passar por todos os exemplos no lote. Isso pode acelerar a aprendizagem aproveitando os rápidos algoritmos de multiplicação de matriz.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
